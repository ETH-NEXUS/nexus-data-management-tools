# This config must live inside your selected drop folder. The tool reads only <drop>/sync.yml.
drop_filename_regex: '(?P<phase>[^/]+)/.*/(?P<uploaded>\d{6}_\d{3}-(?P<hosp_sec>(?P<hosp_pri>[A-Z0-9]+_[A-Z]{1})_[a-z0-9]+-\d)_S\d+_R(?P<read>\d)_\d+\.fastq\.gz)'
drop_filename_filter: '**/*.fastq.gz'
 # Notes about discovery/matching:
 # - The regex is applied to each source path with the drop folder prefix removed.
 # - You can reference named capture groups like <uploaded>, <hosp_sec>, etc. in templates below.
 # - If you need to normalize captured variables (e.g., replace '-' with '.'), use 'replacements.before_match'.
 # - The glob is evaluated relative to the selected drop folder to discover candidate files.
repository_folder: /cluster/work/mtorus/data-repository
 # Template for the destination path relative to repository_folder.
 # Supported placeholders:
 #   Regex groups, e.g. <uploaded>, and derived groups via metadata_derive (see below)
 #   <run>  (auto-incremented when filename_sequence: run)
 #   <hash> (CRC32 of file contents when filename_sequence: hash)
 #   <source.Field> (from matched metadata rows; see metadata_sources and metadata_match)
repository_filename: seq_16S/<phase>/raw/<patient_id>/<patient_id>_<visit>-<sample_id>-<technology>/<patient_id>_<visit>-<sample_id>-<technology>-R<read>r<run>v1.0__<uploaded>
 # Destination parent for archiving/moving original drop files after a successful copy+verify.
 # After a successful copy+verify to the repository, the original file is moved
 # under this folder, preserving its path relative to the selected drop folder.
 # Example:
 #   <drop>/seq_16S/validation/2025-04-08/file.fastq.gz
 # becomes
 #   <processed_folder>/seq_16S/validation/2025-04-08/file.fastq.gz
 # The move is executed only with --do-it; otherwise a dry-run summary is shown.
processed_folder: /cluster/work/mtorus/data-processed

# Filename sequencing for collisions / canonical naming:
# - run  : Replace <run> with an integer starting at 1 and increment if a
#          conflicting target filename is already planned in this run.
# - hash : Replace <hash> with an 8-character uppercase CRC32 of the file
#          contents (e.g., A1B2C3D4). Use this when you want deterministic
#          names without maintaining a per-prefix run counter.
filename_sequence: run

# The date format (strftime) to use for date fields (used by now() and drop_file_mtime()).
date_format: '%Y-%m-%d %H:%M:%S'

# LabKey configuration
# Define:
# - host:     Hostname of the LabKey instance
# - container: Project/folder name in LabKey (e.g., "LOOP Intercept")
# - context:  Optional context path (not currently used by the tool)
# - schema:   Schema containing the table (e.g., exp)
# - table:    Table name (e.g., data or a custom table)
# Important: schema/table must match how LabKey expects them in select_rows(schema, table).
#            If you use a custom table like scRNA_Experiments, ensure 'schema' is your schema
#            name (without dots) and 'table' is the exact table name.
labkey:
  host: ctr-biomed-13.leomed.ethz.ch:8080
  container: 'LOOP mTORUS'
  context:
  use_ssl: false
  # IMPORTANT:
  # - 'schema' must be the schema name only (no dots), e.g., 'exp' or your custom schema
  # - 'table' must be the exact table name in that schema, e.g., '16S_Experiments'
  schema: exp
  table: 16S_Experiments

# Define values for fields in the write-back LabKey table (defined in 'labkey' above).
# You can use placeholders (<phase>, <seq>, <lib>, <prefix>, <suffix>, <run>, <hash>)
# captured by drop_filename_regex. Additionally, the following functions are supported:
# - now()              : Current local date-time formatted per date_format
# - drop_file_mtime()  : Last modification time of the source file (formatted)
# Notes:
# - Non-string values (e.g., booleans) should be written using YAML types (True/False).
fields:
  Name: <samples.Name>-r<run>v1.0__<uploaded>
  Project Phase: Validation
  Reupload Number:
  SOP Version:
  SOP Name:
  Data Synced: True
  Date Of Syncing: now()
  Date Of Uploading: drop_file_mtime()
  Path To Synced Data: 
  Uploaded File Name: <uploaded>
  Technology_Sample_Id: <samples.Name>
  Primary_Sample_Id: <sample_name_prefix>
  Hospital Primary Sample Id:
  Hospital Secondary Sample Id:
  Md5sum: ""

# Define special roles for fields:
# - file_list            : Used now to check LabKey presence via a CONTAINS filter on this field
# - file_list_aggregator : Reserved for future aggregation during write-back
field_parameters:
  Path To Synced Data: file_list

# Reserved for future write-back (not used in current version):
# Lookups translate captured values to another representation.
# Example: Map a regex-captured phase to a canonical label.
# Define as nested keys: lookups.phase.<from-value>: <to-value>
lookups:
  phase:
    btki: BTKi


# Optional: External metadata sources to load before syncing.
# Supported types: labkey, excel, csv
# - labkey: requires schema and table; optional columns, filters. Uses top-level labkey.host/container
# - excel: requires path; optional sheet
# - csv  : requires path; optional delimiter
metadata_sources:
  - name: samples
    type: labkey
    schema: samples
    table: 16S_Sequencing_Samples
    columns: [Name, Hospital Secondary Sample Id]


# Optional: Rules to find the metadata row for each file before syncing.
# The tool renders a key using the named regex variables from the file path
# (e.g., <phase>, <lib>, <seq>, <prefix>, <suffix>) and searches the given
# source's field for an exact string match.
#
# You can specify a default key_template and per-source overrides. The first
# successful rule wins and annotates the file with meta_found, meta_source,
# meta_key, and meta_row.
metadata_match:
  # Use the hospital secondary id from filename; normalization is done via replacements.before_match
  key_template: <hosp_sec>
  search:
    - source: samples
      field: Hospital Secondary Sample Id

# If true, a metadata match is required to allow copying and moving. Files without a
# matching metadata row will be skipped with reason "metadata_missing".
metadata_required: true

# Derive variables from matched metadata to drive folder layout and filenames
metadata_derive:
  - source: samples
    field: Name
    regex: '^(?P<patient_id>MTR-[A-Z]-\d{3})_(?P<visit>V\d{2})-(?P<sample_id>[A-Z]{2}-\d{2}-\d{2})-(?P<technology>[A-Z]*)$'
  - source: samples
    field: Hospital Secondary Sample Id
    regex: '^(?P<hosp_secondary_id>.+)$'
  - source: samples
    field: Name
    regex: '^(?P<sample_name_prefix>MTR-[A-Z]-\d{3}_V\d{2}-[A-Z]{2})'

presence_check:
  # Controls how existing rows are found in the write-back table.
  # Default behavior (if omitted): uses the 'file_list' role to search by target path with CONTAINS.
  # To match by a specific field value, set 'field' to a key present in 'fields' and choose 'match':
  # - equal    : QueryFilter.EQUAL
  # - contains : QueryFilter.CONTAINS
  field: Uploaded File Name
  match: equal       # equal | contains

# Write-back behavior
# - skip_creates: when true, presence check determines updates-only; new rows are NOT inserted.
#   Additionally, when a write-back is skipped (either due to skip_creates or missing RowId to update),
#   the file copy and the final archive/move are skipped as well. Dry-run tables will show 'writeback_skipped'.
writeback:
  skip_creates: true

# Optional: Character/string replacements
# - before_match: applied to captured/derived variables prior to metadata matching and templating
# - before_writeback: applied either to variables (target: var) before rendering or to final fields (target: field)
#   Rule format:
#     - target: var|field
#       name: <variable-or-field-name>
#       replace: <string-to-replace>
#       with: <replacement-string>
replacements:
  before_match:
    - target: var
      name: hosp_sec
      replace: '-'
      with: '.'
  before_writeback:
    # Example: normalize Primary_Sample_Id hyphens to underscores
    - target: field
      name: Primary_Sample_Id
      replace: '-'
      with: '_'
